
# Materials and Methods

## Imaging

MR images utilized for this initial report were acquired from a single scanner involved in the Chronic Effects of Neurotrauma Consortiumâ€™s (CENC) observational study (see Walker et al., this issue). Briefly, participants were Operation Iraqi Freedom/Operation Enduring Freedom (OIF/OEF) era Service Members and Veterans between the ages of 18-60 years with prior combat exposure and deployment(s). The feature images were derived from MR acquisitions of 26 subjects aged 39.6 $\pm$ 8.1 years (range 28--58 years). Within this cohort, 24 (92%) were considered positive for TBI based upon the potential concussive events (PCE) interview process described in detail in Walker et al., this issue. All lesions were isolated in the white matter of the cerebrum.  Table 1 provides a descriptive statistical summary of the variation in lesion
load across the selected cohort.

<!--

\input{trainingData.tex}

-->

Images were acquired on a Philips 3.0T Ingenia system with an 8-channel SENSE head coil (Philips Medical Systems, Best, Netherlands).  3D FLAIR sequences were acquired with a turbo spin echo inversion recovery sequence with the following parameters:  repetition time (TR) = 4800 ms, echo time (TE) = 325 ms, inversion time (TI) = 1650 ms; 170 sagittal slices with a 1.2 mm slice thickness, 256 $\times$ 256 acquisition matrix, and 256 $\times$ 256 mm FOV.  3D T1-weighted sequences were acquired with a fast field echo (FFE) sequence with the following parameters:  TR = 6.8 ms, TE = 3.2 ms, echo train length (ETL) = 240; flip angle = $9^\circ$, 170 sagittal slices with a 1.2 mm slice thickness, 256x240 acquisition matrix, and 256 $\times$ 256 mm FOV.  In addition, 3D T2-weighted images were acquired with a turbo spin echo sequence with the following parameters:  TR = 2500 ms, TE = 245 ms, ETL: = 133; 170 sagittal slices with a 1.2 mm slice thickness, 256 $\times$ 256 acquisition matrix, and 256 x 256 mm FOV.

The first author (J. R. S.) performed the manual WMH tracings for all 26
subjects.  J. R. S. is a radiologist certified by the American Board of Radiology, with a certificate of advanced qualification in vascular and interventional radiology, over 18 years of research experience in TBI, and 6 years of clinical imaging experience.  All
multi-modal MR dicom image slices were converted to the nifti file format [^1].
All nifti image volumes for each subject were rigidly aligned to the T1 image of that subject using the ANTs software [@Avants:2014aa].  The normalized MRI volumes were then provided to
J. R. S. who traced each lesion using the ITK-SNAP tool [@Yushkevich:2006aa]
which has multi-image overlay capabilities for visualizing all modalities in all three
canonical views.

[^1]: http://nifti.nimh.nih.gov/nifti-1


## Quantitative analysis

Figure 1 provides a graphical overview of the proposed workflow.  The major components
include offline generation of symmetric multimodal templates, the creation of
feature images from the training data which are then employed for statistical
prediction using a concatenated random forest framework.  This framework involves
the use of two random forest models where the outcome (i.e., the tissue
probability estimates) of the first RF model application,
which we denote as "Stage 1", is used as input (along with the original set of
feature images) to a refinement RF model segmentation which we denote as "Stage 2".
Once these offline steps
are performed, a new (i.e., unsegmented) subject can then be processed using the proposed pipeline.

<!--

![Workflow illustration for the proposed pipeline.  Processing of the multi-modal
input MRI for a single subject, using the multi-modal symmetric template, results in
the generation of the feature images.  These feature images are used as input to the
Stage 1 RF model producing the initial RF probability map estimates.  The Stage 1
voting maps, the original feature images, and the Stage 2 RF model result in the
final voting maps which includes the WMH probability estimate.  Note that the RF models
are constructed once from a set of training data which are processed using the
same feature-construction pipeline as the single-subject input MRI.](Figures/wmhPipeline.png)

-->

### Symmetric multi-modal templates

Following [@Tustison:2014ab] and [@Tustison:2015aa], optimally derived templates
serve for both brain tissue segmentation (for the derivation of tissue prior probability maps) and generation of asymmetry feature images.
For this work we use the multi-modal data available from the public MMRR data set
[@landman2011].  We used all 46 multi-modal acquisitions of that study to produce a multi-modal template according to the procedure described in [@Avants:2010aa] which
results in a mean (in terms of both shape and intensity) multivariate template representing the entire cohort.  Mid-canonical slices of the FLAIR, T1, and T2 components are illustrated in Figure 2.

<!--

![Canonical views of the mutlivariate, bilaterally symmetric template constructed
from the MMRR data set [@landman2011] (only shown are the FLAIR, T1, and T2 modalities---
the components relevant for this work).  Template construction is detailed in
[@Tustison:2015aa].  These images are important for specific asymmetry-based
features.](Figures/MMRR.png)

-->

### Feature images for WMH segmentation


Crucial to these supervised segmentation approaches are the creation and selection of
"features" as input (i.e., feature images constructed from the training data)
in conjunction with expertly identified structures of interest
(i.e., WMHs) for model construction.  For the targeted application in this work,
tissue classification is performed at the voxelwise level.  In other words, each voxel
within the region of interest is sent through the ensemble of decision trees and receives
a set of classification votes from each tree thus permitting a regression or classification
solution.  Since this procedure is performed at the voxelwise level, intensity information
alone is insufficient for good segmentation performance due to the lack of spatial context.
For example, as pointed out in [@Neema:2009aa], higher intensities can be found at the
periventricular caps in normal subjects which often confounds automated lesion detection
algorithms.  Other potential confounds include MR signal inhomogeneity and noise.  Therefore,
even though machine learning and pattern recognition techniques are extremely powerful and
have significant potential, just as crucial to outcome is the creative construction and
deployment of salient feature images which we detail below.

<!--

\input{featureImagesTable.tex}

-->

Supervised methodologies are uniquely characterized, in part, by the feature images that
are used to identify the regions of interest.  In Table 2, we provide a list and basic
categorization of the feature images used for the initial (i.e., Stage 1---more on the use
of multiple random forest stages below) segmentation of the WMHs.
In addition Figure 3 provides a representation of a set of feature images for a single
subject analyzed in this work.  Note that in this work we categorize the brain parenchyma
with seven labels:

* cerebrospinal fluid (label 1),
* gray matter (label 2),
* white matter (label 3),
* deep gray matter (label 4),
* brain stem (label 5),
* cerebellum (label 6), and
* white matter hyperintensities (label 7).

<!--

![Representation of Stage 1 feature images for subject 9.  The
FLAIR, T1-, and T2-weighted images are rigidly pre-aligned
[@Avants:2014aa] to the space of the FLAIR image.  The three modality images are then preprocessed
(N4 bias correction [@Tustison:2010ac] and adaptive denoising [@Manjon:2010aa]) followed by
application of standard ANTs brain extraction and $n$-tissue segmentation protocols using
the MMRR symmetric template and corresponding priors [@Tustison:2014ab] applied to the T1 image.
The feature images are then generated for voxelwise input to the RF model which results in
the voting maps illustrated on the right.  This gives a probabilistic classification of
tissue type.  Not shown are the probability and voting images for the brain stem and
cerebellum.](Figures/featureImages.png)

-->

As mentioned previously, input for each subject comprises FLAIR, T1-, and T2-weighted
acquisitions.  The T1 and T2 images are rigidly registered to the FLAIR image using
the open-source Advanced Normalization Tools (ANTs) [@Avants:2014aa].  The aligned images
are then preprocessed using the denoising algorithm of [@Manjon:2010aa] followed by
N4 bias correction [@Tustison:2010ac] which are then normalized to the intensity
range $[0,1]$.  Although we could have used an alternative intensity standardization
algorithm (e.g., [@nyul2000]), we found that a simple linear rescaling produced better results similar to previous work[@Tustison:2015aa].

<!--

![Sample FLAIR acquisition image slices showing both manual and random forest
segmentations for both stages obtained during the leave-one-out evaluation.  Manual
segmentations were performed by one of the authors and provided the ground
truth WMH labels for training the random forest models.](Figures/sampleResults.png)

-->

The T1 image is then processed via the ANTs brain
extraction and normal tissue segmentation pipelines [@Tustison:2014ab].
  The result is a mask delineating the brain parenchyma and probabilistic estimates of the
CSF, gray matter, white matter, deep gray matter, brain stem, and
cerebellum [@Avants:2011aa].  These provide the expertly annotated labels for the
first six tissue labels given above.  Tissue prior probability maps for segmentation
are from multi-model optimal symmetric shape/intensity templates [@Tustison:2015aa] created from the public MMRR data set  [@landman2011] (cf Figure 2).


Feature values include the preprocessed FLAIR, T1, and T2 image voxel intensities.
We also calculate a set of neighborhood statistics (mean, standard deviation, and skewness) feature images using
a Manhattan radius of one voxel given the typical size
of individual WMHs. For each of the preprocessed
images, we calculate the difference in intensities with the corresponding warped template
component.  Previous success in the international brain tumor segmentation
competition [@Menze:2015aa] was based on an important set of intensity features that were created
from multi-modal templates mentioned previously [@Tustison:2015aa] and listed in Table 2.  We employ the
same strategy here.

<!--
For example, the template
difference feature image for the FLAIR image, $S_{FLAIR}$ is calculated as:

$$S_{FLAIR} - T_{FLAIR}\left(\phi_b^{-1}\right)$$

where

$$\phi_b: S  \leftrightarrow \underset{b}{\leftrightsquigarrow} T$$

is the transform
which maps from the individual subject space to the
template space and $T_{FLAIR}$ is the FLAIR template component.
-->

To take advantage of the gross bilateral symmetry of the normal brain (in terms of both shape and intensity), and the fact that WMHs do not generally manifest symmetrically across hemispheres,
we use the symmetric templates to compute the contralateral intensity
differences as an additional intensity feature.

<!--
For the FLAIR component, this contralateral
difference image is calculated from

$$S_{FLAIR} - S_{FLAIR}\left(\phi_b^{-1}\left(\phi_R\left(\phi_b\right)\right)\right)$$

where $\phi_R$ denotes a horizontal reflection perpendicular to the mid-sagittal plane of
the symmetric template.
-->

The segmentation probability images described above are used as feature images to
provide a spatial context for the random forest model prediction step.  Additional spatial contextual
feature images include the distance maps [@maurer2003] based on the csf, gray matter, and
deep gray matter images.  These latter images are intended to help distinguish white matter
hyperintensities from false positives induced by the partial voluming at the gray/white
matter interface.  A third set of images are based on the voxel location within the
space of the template.
Similar feature images were used in
[@Anbeek:2004aa]
although, unlike the proposed framework, this previous work lacks
normalization to the standard
coordinate system provided by the template to dramatically improve spatial specificity
across all subjects.  To generate these images, the T1 image of each subject is
registered to the T1 template component using a B-spline variant [@Tustison:2013ac]
of the well-known ANTs Symmetric Normalization (SyN) algorithm [@Avants:2011ab].
Using the derived transforms, the template coordinate images are warped back to the space of the individual subject.


### Stacked (concatenated) random forests for improved segmentation performance

In previous brain tumor segmentation work [@Tustison:2015aa], it was demonstrated that a
concatenated supervised approach, whereby the prediction output from the first random forest
model serves as partial input for a second random forest model, can significantly improve
segmentation performance.  We do the same thing for the work described here where we employ two stacked random forests (or two "stages").  The Stage 1
feature images of the training data (as described previously) are used to construct the
Stage 1 model.  The training data Stage 1 features are then used to produce the voxelwise
"voting maps" (i.e., the classification count of each decision tree for each tissue label) via the Stage 1 random forest model.  All the Stage 1 features plus the Stage 1 voting maps
are used as input to the Stage 2 model.  In addition, we use the Stage 1 voting maps as
tissue priors (i.e., probabilistic estimates of the tissue spatial locations) for a second application of the $6$-tissue segmentation algorithm with an
additional Markov Random Field spatial prior (MAP-MRF) [@Avants:2011aa].
In order to maximize the spatial information for the $n$-tissue segmentation process following the voxelwise RF classification of Stage 1, we use
all three aligned preprocessed images for multivariate segmentation during the
second stage.
The resulting seven posterior probability images constitute a third additional feature image set for Stage 2.

### Implementation

As pointed out in a recent comprehensive lesion segmentation review
[@Garcia-Lorenzo:2013aa], although the number of algorithms reported in the literature
is quite extensive, there were only four publicly available segmentation algorithms
at the time of writing this article.  In contrast to the current work, none are based on supervised learning.  As we did for
our brain tumor segmentation algorithm [@Tustison:2015aa], all of the code described in
this work is publicly available through the open-source ANTs/ANTsR toolkits.  Through
ANTsR (an add-on toolkit which, in part, bridges ANTs and the R statistical project) we
use the _randomForest_ package [@liaw2002] using the default settings with
2000 trees per model and 500 randomly selected samples per label per image.
Note that we saw little variation in performance when these parameters were changed (i.e. up to 1000 random samples and as little as 1000 trees) which is consistent with our previous experience.

In addition, similar to our previous offering [^2], we plan on creating
a self-encapsulated example to showcase the proposed methodology
which will also be available on github.[^3]  The fact that
the data will also be made available through the Federal Interagency Traumatic Brain Injury Research (FITBIR) repository along
with the manual labelings will facilitate reproducibility on the part of the reader
as well as any interest in extending the proposed framework to other data sets.

[^2]: https://github.com/ntustison/ANTsAndArboles

[^3]: https://github.com/ntustison/WatchMeHyperventilate

### Evaluation protocol overview

In order to evaluate the protocol described, we performed a leave-one-out evaluation using
the data acquired from the 24 subjects described above.  Initial processing included the creation of all Stage 1 feature images for all subjects.  The initial brain segmentation of each T1 image and the manual white matter hyperintensity tracings were combined to provide the truth labels for the training data.  The "truth" labels are the seven anatomical regions given above.

The leave-one-out procedure is as follows:

* Create Stage 1 feature images for all 24 subjects.
* For each of the 24 subjects:
    + sequester the current subject and corresponding feature images.
    + construct the Stage 1 random forest model from the remaining 23 subjects.
    + apply the Stage 1 random forest model to the feature images of the 23 training subjects.
    + the previous step produces the Stage 1 voting maps for all seven labels.
    + for each of the 23 subjects, perform a Bayesian-based segmentation with an
      MRF spatial
      prior using the seven voting maps as additional tissue priors.
    + construct the Stage 2 random forest model from all the Stage 1 feature images,
      seven voting maps, and seven posterior probability maps from the previous step.
    + send the sequestered subject through the random forest models for both stages.
    + compare the final results with the manually-defined white matter hyperintensity
      regions.

Several measures have been employed in the literature for evaluating
automated white matter lesion segmentation involving such quantities as lesion load,
voxel-based overlap measures (such as the Dice similarity coefficient), and lesion-based measures.[@Garcia-Lorenzo:2013aa].  For this work, due to the relatively small-size distribution of the lesion load in our data cohort (see Table 1), we used
four lesion-based measures:  sensitivity, positive predictive value (i.e., precision), $F_1$ score, and relative volume difference.  The first three quantities are based on the number of false positives \textcolor{blue}{($FP$), true negatives ($TN$)},
false negatives ($FN$), and true positives ($TP$) in terms of identified lesions.  It should be noted that the number of true negatives is not readily
incorporated into measures of accuracy as the quantity of "true negatives" (i.e., correctly identified normal brain tissue) would severely skew accuracy assessments. The $F_1$ score is an assessment of accuracy which takes into account both the sensitivity,

$$ sensitivity =  \frac{ TP }{TP + FN}, $$

and the positive predictive value (PPV),

$$ PPV =  \frac{ TP }{TP + FP}, $$

such that $F_1$ is given by

$$ F_1 =  \frac{ 2 \cdot TP }{ 2 \cdot TP + FP + FN}. $$

The relative volume difference is calculated for each of the true positive lesions using
the manual and predicted lesion volumes:

$$ Relative\,\,volume\,\,difference = \frac{V_{manual} - V_{predicted}}{V_{manual}}. $$

To illustrate how the performance of our framework varies with lesion size, we calculated
the above measures based on evenly split quantiles of the manual estimates of lesion volumes into 3 groups.  These three size ranges (in terms of the number of voxels) are:

* Quantile 1: $[1-12)$,

* Quantile 2: $[12-28)$, and

* Quantile 3: $[28-551]$.

These quantiles are used to showcase the performance (cf Figure 5).

