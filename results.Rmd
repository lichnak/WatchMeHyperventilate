
# Results

## Ranking feature importance

After performing the leave-one-out evaluation\deleted[id=dt]{described at the end of the previous section},
we calculated the \texttt{MeanDecreaseAccuracy} feature values for each of the 24 subjects $\times$
2 models per subject $=48$ total models.  This measure (per feature, per model) is calculated
during the out-of-bag phase of the random forest model construction and quantifies the decrease
in prediction accuracy from omitting the specified feature.  In other words, this quantity
helps determine the importance of a particular feature and, although we save such
efforts for future work, this information provides us with guidance for future feature
pruning and/or additions.

![Average \texttt{MeanDecreaseAccuracy} plots generated from the creation of all 24 random
forest models for Stage 1 during the leave-one-out evaluation.  These plots
are useful in providing a quantitative assessment of the predictive importance of each feature.
\added[id=nt]{Features are ranked in descending order of importance.}
The horizontal error bars provide the $95^{th}$ percentile
\deleted[id=bt]{(i.e., $1.96 \times \sigma$)} and illustrate the
stability of the feature importance across the leave-one-out models.
\added[id=nt]{At this initial stage only 31 features images are used.}](Figures/averageLeaveOneOutStage1.png)

![Average \texttt{MeanDecreaseAccuracy} plots generated from the creation of all 24 random
forest models for Stage 2 during the leave-one-out evaluation.  These plots
are useful in providing a quantitative assessment of the predictive importance of each feature.
\added[id=nt]{Features are ranked in descending order of importance.}
The horizontal error bars provide the $95^{th}$ percentile
\deleted[id=bt]{(i.e., $1.96 \times \sigma$)} and illustrate the
stability of the feature importance across the leave-one-out models.
\added[id=nt]{We augment the 31 feature images from the first stage by adding an additional
7 voting maps and 7 segmentation posteriors from application of the Bayesian-based
segmentation for a total of 45 images for the second stage.}](Figures/averageLeaveOneOutStage2.png)



The resulting rankings for both Stages are given in Figures 4 and 5 where the values for the
separate stages are averaged over the entire corresponding model set.  In addition, we
track the variance for each feature over all models to illustrate the stability of
the chosen features during the evaluation.  This latter information is illustrated as
horizontal errors bars providing the $95^{th}$ percentile \deleted[id=bt]{(i.e., $1.96 \times \sigma$)}.
Note that the reader can cross reference Table 1 for identifying corresponding feature types
and names.

One can also use these measurements as a type of sanity check.  For example, from the Stage
1  plot, one can see that the \texttt{MeanDecreaseAccuracy} values for the location indices in the
anterior-posterior direction (i.e., \texttt{TemplateIndicesWarped1}) are greater than
those for either the inferior-superior (i.e., \texttt{TemplateIndicesWarped0}) or the
left-right (i.e., \texttt{TemplateIndicesWarped0}) directions in the space of the symmetric
template.  \deleted[id=bt]{This is intuitive since, as discussed previously, manifestation of TBI white
matter hyperintensities can often be confused with higher intensities at the
periventricular caps in normal subjects [@Neema:2009aa]} \deleted[id=am]{whereas there does not seem to
be contralateral bias in manifestation of white matter hyperintensities in TBI.}

Additionally, it is interesting to note some of the other top performing features for Stage 1.
The contralateral difference FLAIR image is  highly discriminative over the set of
evaluation random forest models \added[id=bt]{(see Figure 6)}.  This accords with the known clinical relevance of
FLAIR images for identifying white matter hyperintensities and the fact that such
pathology does not \added[id=nt]{typically} manifest symmetrically in both hemispheres.  Interestingly, the
posterior maps for the deep gray matter are extremely important for accurate white matter
hyperintensity segmentation.  Perhaps the spatial specification of deep
gray matter aids in the removal of false positives.  Inspection of the bottom of the
plots demonstrates the lack of discriminating features associated with the
T1 image which is also well-known in the clinical literature.

![\added[id=nt]{(a) FLAIR image slice illustrating WMHs which have been manually delineated
in (b).  (c) Contralateral FLAIR difference image.}](Figures/FLAIRcontralaleteralWithLesions.png)

As described earlier, for Stage 2, we used the output random forest voting maps from Stage
1 as both features themselves and as priors for input to a Bayesian-based segmentation with
an additional MRF spatial prior.  In Figure 5, the voting maps are labeled as
"\texttt{RFStage1VotingMaps}" where the final numeral is associated with the brain
parenchymal labeling given previously.  Similarly, the additional RF prior segmentation
feature probability maps are labeled as "\texttt{RFBrainSegmentationPosteriors}".
The Stage 2 feature importance plot follows similar
trends as that for Stage 1 with the T1 images not contributing much to the identification
of white matter hyperintensity voxels.  The initial voting maps from Stage 1 are extremely
important with the top 3 being the estimated locations of the 1) gray matter, 2) white matter, and
3) white matter hyperintensities.  Since these tissue type can be conflated based on intensity
alone it is intuitive that such features would be important.

## White matter hyperintensity segmentation evaluation

In Figure 7 are the segmentation comparisons derived from manual segmentations of the same
data.  Despite the large variability characteristic with manual labelings in related fields
[@Grimaud:1996aa;@styner2008;@Garcia-Lorenzo:2013aa], such labelings are characteristic of
current clinical practices and the methodology proposed herein is readily adapted to refinements
in training data.  On the left of Figure 7 are the improvement in Dice values [@tustison2009],
\added[id=nt]{i.e.,}

$$ Dice = 2 \frac{\sum_r|S_r \cap T_r|}{\sum_r|S_r| + |T_r] } $$

\added[id=nt]{over all white matter hyperintensities when comparing the segmentations between the two stages
where the sum is taken over all individually labeled manual, $T_r$, and automated, $S_r$, lesions and $\cap$
represents the intersection between the manual/automated lesion pair.}  Performing
the second round of supervised learning improves these Dice values.  One can also note from the
right side of Figure 6 that the total lesion load volume illustrates a few subjects that are
severe outliers in terms of the number of false positives.  The second round helps to
correct this issue.

![\added[id=am]{Voxelwise} comparison with manual delineation of white matter hyperintensities.  On the left are the
calculate\added[id=am]{d} Dice values over all white matter hyperintensities.  Note the improvement
in the Dice metric from the employment of the Stage 2 component of the processing pipeline.
(Right) Similar results can be seen by comparing the total lesion load volume between manual
and automated detection strategies.  Although some outliers are found after the Stage 2
processing in a couple subjects, the number of outliers caused by false positives is
\added[id=lw]{decreased} significantly with the second stage processing.](Figures/llvAndDice.png)

